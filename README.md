## Computer Vision Project Fall 2021, NYU Courant
### Image Style Transfer: Two Methods

Style transfer is to take an input image as the target style,an input content image to convert, then generate the combi-nation of the two to produce artistically styled results.  Wehave  studied  two  methods  of  doing  so.   One  uses  a  pre-trained model to generate the output and takes several min-utes,  commonly known as neural style transfer;  the otherone trains a model on a given style image and generates theoutput within seconds, known as real-time style transfer.For neural style transfer,  we used pre-trained VGG-16and  VGG-19.   We  experimented  with  different  configura-tions and found that VGG-19 with Adam optimizer yieldsthe  best  result.For  VGG-16  with  1000  epochs  usingLBFGS,  generating  output  for  our  given  example  here  ofsize 681*968 takes  2min with a Tesla P100, and VGG-19with Adam optimizer with 4000 epochs takes around halfan hour. Even though this method can generate visually ap-pealing results, speed becomes a major concern.For  real-time  style  transfer,  we  first  used  pre-trainedVGG-16 to extract features of the style features, then builta  transformation  model  with  in-network  upsampling  anddownsampling. We experimented with different datasets andfound out that for this task, larger datasets generate a betterresult.  Once the model is trained (time taken varies on thesize of the dataset), generating a result for a content pictureof size 681*968 takes 0.52s on a Tesla P100

This is the code repository for [our paper](https://github.com/MagicShow1999/CV-Final-Project/blob/main/Neural-Style-Transfer.pdf)

## Group memeber: Haodong Wu, Xiaoya Wang, Yinuo Zhang
